import os
import json
import pickle
import chromadb
import google.generativeai as genai
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# T√©l√©charger les ressources n√©cessaires pour nltk
import nltk
nltk.download('punkt')
nltk.download('wordnet')

# Initialisation des ressources
lemmatizer = WordNetLemmatizer()

# Charger les mod√®les et le vectorizer
def load_models():
    with open("model.pkl", "rb") as model_file:
        model = pickle.load(model_file)
    with open("vectorizer.pkl", "rb") as vectorizer_file:
        vectorizer = pickle.load(vectorizer_file)
    with open("label_encoder.pkl", "rb") as encoder_file:
        label_encoder = pickle.load(encoder_file)

    # Initialisation de ChromaDB pour la recherche de questions complexes
    client = chromadb.Client()
    collection = client.get_or_create_collection(name="questions_attijari_wafa")
    
    return model, vectorizer, label_encoder, collection

# Pr√©traitement de la phrase
def preprocess(sentence):
    tokens = word_tokenize(sentence.lower())  # Tokenisation
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum()]  # Lemmatisation
    return " ".join(tokens)

# Fonction pour obtenir une r√©ponse depuis ChromaDB
def get_direct_response_from_chromadb(question, collection, vectorizer):
    # Pr√©traiter la question
    preprocessed_question = preprocess(question)
    
    # Vectoriser la question (transformer en embedding avec le m√™me vectorizer utilis√© lors de l'indexation)
    question_embedding = vectorizer.transform([preprocessed_question]).toarray()

    # Rechercher dans ChromaDB avec l'embedding
    search_results = collection.query(query_embeddings=question_embedding.tolist(), n_results=1)
    
    # Si des r√©sultats sont trouv√©s, retourner le document associ√© comme r√©ponse
    if search_results["documents"]:
        document = search_results["documents"][0]
        return f"R√©ponse trouv√©e dans la base de donn√©es : {document}"  # Fournir plus de d√©tails sur la source de la r√©ponse
    
    return "D√©sol√©, je n'ai pas trouv√© de r√©ponse directe √† votre question dans la base de donn√©es ChromaDB."  # Si aucun r√©sultat n'est trouv√©


# Fonction pour d√©tecter si la question est complexe en la recherchant dans ChromaDB
def is_complex_question(question, collection, vectorizer):
    # Pr√©traiter la question
    preprocessed_question = preprocess(question)
    
    # Vectoriser la question (transformer en embedding avec le m√™me vectorizer utilis√© lors de l'indexation)
    question_embedding = vectorizer.transform([preprocessed_question]).toarray()

    # Rechercher dans ChromaDB avec l'embedding
    search_results = collection.query(query_embeddings=question_embedding.tolist(), n_results=1)
    
    # Si la recherche retourne des r√©sultats, c'est une question complexe
    if search_results["documents"]:
        return True
    return False

# Utiliser Gemini pour g√©n√©rer une r√©ponse pour une question complexe
def generate_response_with_gemini(question):
    # R√©cup√©ration de la cl√© API √† partir de l'environnement
    GOOGLE_API_KEY = "YOUR_API_KEY"
    
    if not GOOGLE_API_KEY:
        raise ValueError("La cl√© API Google n'est pas d√©finie dans le fichier .env")

    try:
        # Configuration de l'API Gemini avec la cl√© API charg√©e
        genai.configure(api_key=GOOGLE_API_KEY)

        generation_config = {
            "temperature": 1,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 8192,
            "response_mime_type": "text/plain",
        }

        model = genai.GenerativeModel(
            model_name="gemini-1.5-pro",
            generation_config=generation_config,
           system_instruction= """
            Vous √™tes AWBBot un assistant IA expert en services bancaires et d'assurances. Votre mission est de fournir des r√©ponses claires, pr√©cises et professionnelles aux utilisateurs en utilisant deux principales sources :

            1. intents.json : Ce fichier contient les questions fr√©quentes et leurs r√©ponses pr√©d√©finies pour les requ√™tes simples.
            2. info.pdf : Ce document est utilis√© pour enrichir les r√©ponses complexes n√©cessitant des d√©tails suppl√©mentaires.

            ### Instructions de fonctionnement :
            1. **Correspondance des intentions :**
            - Si une question correspond √† un "pattern" pr√©sent dans intents.json, utilisez directement la r√©ponse pr√©d√©finie.

            2. **Requ√™tes complexes :**
            - Si aucune r√©ponse directe n'est trouv√©e, consultez le contenu du document info.pdf.
            - Formulez une r√©ponse structur√©e en incluant :
                - Une introduction claire.
                - Des informations d√©taill√©es avec des exemples chiffr√©s lorsque possible.
                - Une r√©f√©rence utile (contact/service client) si applicable.

            3. **Ambigu√Øt√© :**
            - Si la question est inconnue ou hors sujet ou manque de pr√©cision, demandez √† l'utilisateur de clarifier sa demande.

            4. **Ton et style :**
            - Soyez professionnel, concis et utilisez un langage adapt√© aux services bancaires et d'assurances.

            ### Format attendu des r√©ponses :
            - **Questions simples :** R√©ponse directe depuis intents.json.
            - **Questions complexes :** R√©ponse enrichie avec des d√©tails extraits de info.pdf.
            """
        )

        chat_session = model.start_chat(history=[])
        response = chat_session.send_message(question)
        return response.text

    except Exception as e:
        print(f"Erreur lors de la g√©n√©ration de la r√©ponse avec Gemini : {e}")
        return "D√©sol√©, une erreur est survenue lors de la g√©n√©ration de la r√©ponse."


# R√©pondre √† la question de l'utilisateur en recherchant directement dans ChromaDB
def respond_to_user_input(user_input, collection, model, vectorizer, label_encoder):
    # Pr√©traiter l'entr√©e de l'utilisateur
    preprocessed_input = preprocess(user_input)

    # V√©rifier si la question correspond √† un "pattern" dans intents.json (r√©ponse simple)
    with open("intents.json", "r", encoding="utf-8") as file:
        intents = json.load(file)
            
    for intent in intents["intents"]:
        for pattern in intent["patterns"]:
            if preprocessed_input == preprocess(pattern):  
                # V√©rifier si la question est complexe
                if intent.get("complexe", False):
                    print("üß† Question complexe d√©tect√©e, r√©ponse g√©n√©r√©e par Gemini...")
                    return generate_response_with_gemini(user_input)
                else:
                    return intent["responses"][0]

    # Si la question n'est pas simple, v√©rifier si c'est une question complexe
    if is_complex_question(preprocessed_input, collection, vectorizer):
        print("üß† Question complexe d√©tect√©e, recherche directe dans ChromaDB...")
        return get_direct_response_from_chromadb(user_input, collection, vectorizer)
    
    # Si la question n'est ni simple ni complexe, essayer de pr√©dire avec le mod√®le
    print("üìù Question simple, r√©ponse bas√©e sur l'intention...")
    # Transformer l'entr√©e utilisateur et pr√©dire l'intention
    X_input = vectorizer.transform([preprocessed_input])
    prediction = model.predict(X_input)
    intent = label_encoder.inverse_transform(prediction)[0]
    return intent  # Renvoie l'intention comme r√©ponse


if __name__ == "__main__":
    # Charger les mod√®les et ChromaDB
    model, vectorizer, label_encoder, collection = load_models()
    
    # Tester avec une entr√©e utilisateur
    user_input = "Comment puis-je r√©duire mes frais bancaires en utilisant vos services ?"
    response = respond_to_user_input(user_input, collection, model, vectorizer, label_encoder)
    print(f"R√©ponse de l'assistant : {response}")
